{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyNQGXU9pHDCDcQCsb+XVmTE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install tavily-python langchain-tavily langgraph langgraph-sdk langchain openai chromadb tiktoken bs4 python-dotenv python-whois xmltodict textblob transformers yfinance\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQ6Jw3jUb5ZX","executionInfo":{"status":"ok","timestamp":1745416530665,"user_tz":-330,"elapsed":47860,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"dc80d85a-8d4a-4513-bd16-efbbf462d7c9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tavily-python\n","  Downloading tavily_python-0.5.4-py3-none-any.whl.metadata (91 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/91.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m81.9/91.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.6/91.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-tavily\n","  Downloading langchain_tavily-0.1.5-py3-none-any.whl.metadata (11 kB)\n","Collecting langgraph\n","  Downloading langgraph-0.3.31-py3-none-any.whl.metadata (7.9 kB)\n","Collecting langgraph-sdk\n","  Downloading langgraph_sdk-0.1.63-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n","Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n","Collecting chromadb\n","  Downloading chromadb-1.0.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Collecting tiktoken\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting bs4\n","  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n","Collecting python-dotenv\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Collecting python-whois\n","  Downloading python_whois-0.9.5-py3-none-any.whl.metadata (2.6 kB)\n","Collecting xmltodict\n","  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tavily-python) (2.32.3)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.28.1)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (3.11.15)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (0.3.52)\n","Collecting mypy<2.0.0,>=1.15.0 (from langchain-tavily)\n","  Downloading mypy-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n","Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n","  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n","Collecting langgraph-prebuilt<0.2,>=0.1.8 (from langgraph)\n","  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n","Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk) (3.10.16)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n","Collecting build>=1.0.3 (from chromadb)\n","  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n","Collecting chroma-hnswlib==0.7.6 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n","Collecting fastapi==0.115.9 (from chromadb)\n","  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n","Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n","Collecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n","Collecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n","Collecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n","Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n","Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n","  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.4)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from python-whois) (2.8.2)\n","Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n","Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n","Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n","Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n","Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n","Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.19.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n","Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n","  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (1.0.8)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n","  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (1.33)\n","Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n","  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Collecting mypy_extensions>=1.0.0 (from mypy<2.0.0,>=1.15.0->langchain-tavily)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n","Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n","Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n","Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.4.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-tavily) (3.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","Downloading tavily_python-0.5.4-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_tavily-0.1.5-py3-none-any.whl (14 kB)\n","Downloading langgraph-0.3.31-py3-none-any.whl (145 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.2/145.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_sdk-0.1.63-py3-none-any.whl (47 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chromadb-1.0.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n","Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Downloading python_whois-0.9.5-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n","Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n","Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n","Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n","Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n","Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n","Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n","Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n","Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n","Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=b86f5e577c01eb5a746b9530df9a0acae71267202b97d6a64b9cdc461f408afc\n","  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n","Successfully built pypika\n","Installing collected packages: pypika, monotonic, durationpy, xxhash, xmltodict, uvloop, uvicorn, python-dotenv, pyproject_hooks, overrides, ormsgpack, opentelemetry-util-http, opentelemetry-proto, mypy_extensions, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, tiktoken, starlette, python-whois, posthog, opentelemetry-exporter-otlp-proto-common, mypy, coloredlogs, build, bs4, tavily-python, onnxruntime, langgraph-sdk, kubernetes, fastapi, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langgraph-checkpoint, opentelemetry-instrumentation-fastapi, langgraph-prebuilt, langgraph, langchain-tavily, chromadb\n","Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 bs4-0.0.2 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.6 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 langchain-tavily-0.1.5 langgraph-0.3.31 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.63 mmh3-5.1.0 monotonic-1.6 mypy-1.15.0 mypy_extensions-1.1.0 onnxruntime-1.21.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-util-http-0.53b1 ormsgpack-1.9.1 overrides-7.7.0 posthog-3.25.0 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 python-whois-0.9.5 starlette-0.45.3 tavily-python-0.5.4 tiktoken-0.9.0 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5 xmltodict-0.14.2 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["!pip install pytest pytest-mock\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L6Bnppjgidzs","executionInfo":{"status":"ok","timestamp":1745416537463,"user_tz":-330,"elapsed":6783,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"5d9b918b-bdd5-4f1b-f78f-516996a2802a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.3.5)\n","Collecting pytest-mock\n","  Downloading pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest) (2.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest) (24.2)\n","Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest) (1.5.0)\n","Downloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n","Installing collected packages: pytest-mock\n","Successfully installed pytest-mock-3.14.0\n"]}]},{"cell_type":"code","source":["from huggingface_hub import login\n","\n","# Enter your Hugging Face token here\n","login(\"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\")\n"],"metadata":{"id":"ynjXFkvP_GEI","executionInfo":{"status":"ok","timestamp":1745416538999,"user_tz":-330,"elapsed":1535,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["pip install tavily-python\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgbY65dBvSZG","executionInfo":{"status":"ok","timestamp":1745416543547,"user_tz":-330,"elapsed":4550,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"468bd85e-ac90-48dd-d2b3-e518393bcd62"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tavily-python in /usr/local/lib/python3.11/dist-packages (0.5.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tavily-python) (2.32.3)\n","Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.9.0)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.28.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (2025.1.31)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (1.0.8)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->tavily-python) (1.3.1)\n","Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->tavily-python) (4.13.2)\n"]}]},{"cell_type":"code","source":["!pip install -U langchain-community\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOvTeUvjD0A7","executionInfo":{"status":"ok","timestamp":1745416566011,"user_tz":-330,"elapsed":22462,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"32cc9a15-36d0-4196-9a41-5b60490ddbfb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-community\n","  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n","Collecting langchain-core<1.0.0,>=0.3.55 (from langchain-community)\n","  Downloading langchain_core-0.3.55-py3-none-any.whl.metadata (5.9 kB)\n","Collecting langchain<1.0.0,>=0.3.24 (from langchain-community)\n","  Downloading langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.31)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (4.13.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n","Downloading langchain_community-0.3.22-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain-0.3.24-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.55-py3-none-any.whl (434 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Installing collected packages: typing-inspect, marshmallow, httpx-sse, dataclasses-json, pydantic-settings, langchain-core, langchain, langchain-community\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.52\n","    Uninstalling langchain-core-0.3.52:\n","      Successfully uninstalled langchain-core-0.3.52\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.23\n","    Uninstalling langchain-0.3.23:\n","      Successfully uninstalled langchain-0.3.23\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.24 langchain-community-0.3.22 langchain-core-0.3.55 marshmallow-3.26.1 pydantic-settings-2.9.1 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["%%bash\n","cat <<EOF > .env\n","TAVILY_API_KEY=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\n","LANGGRAPH_API_KEY=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\n","OPENAI_API_KEY=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\n","EOF\n","\n"],"metadata":{"id":"Brv1670HgXQM","executionInfo":{"status":"ok","timestamp":1745416566064,"user_tz":-330,"elapsed":49,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!pip install deep-translator\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yvk-GOaxvdzD","executionInfo":{"status":"ok","timestamp":1745416571432,"user_tz":-330,"elapsed":5363,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"97cf3ac0-99ee-4e3c-8ebb-74501b8b8b26"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deep-translator\n","  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.4)\n","Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n","Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: deep-translator\n","Successfully installed deep-translator-1.11.4\n"]}]},{"cell_type":"code","source":["from dotenv import load_dotenv\n","import os\n","\n","\n","load_dotenv(dotenv_path=\".env\")\n","\n","# Verify\n","print(\"TAVILY_API_KEY =\", os.getenv(\"TAVILY_API_KEY\"))\n","print(\"LANGGRAPH_API_KEY =\", os.getenv(\"LANGGRAPH_API_KEY\"))\n","print(\"OPENAI_API_KEY =\", os.getenv(\"OPENAI_API_KEY\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJ02fd1JgYrl","executionInfo":{"status":"ok","timestamp":1745416571477,"user_tz":-330,"elapsed":32,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"8587fa82-a329-4fc1-abab-14434f0638f3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["TAVILY_API_KEY = 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\n","LANGGRAPH_API_KEY = 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\n","OPENAI_API_KEY = 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\n"]}]},{"cell_type":"code","source":["%%bash\n","# 1) Install the correct Tavily SDK and LangChain‑OpenAI integration\n","pip install -U tavily-python langchain-openai\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxU8KjgOFonK","executionInfo":{"status":"ok","timestamp":1745416576947,"user_tz":-330,"elapsed":5466,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"d02197b2-9cc0-4dea-99e1-929acbb75d8c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tavily-python in /usr/local/lib/python3.11/dist-packages (0.5.4)\n","Collecting langchain-openai\n","  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tavily-python) (2.32.3)\n","Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.9.0)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.28.1)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.53 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.55)\n","Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.75.0)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.3.31)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (9.1.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (6.0.2)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (4.13.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.11.3)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (1.0.8)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (2.3.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.4.0)\n","Downloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.4/62.4 kB 2.1 MB/s eta 0:00:00\n","Installing collected packages: langchain-openai\n","Successfully installed langchain-openai-0.3.14\n"]}]},{"cell_type":"code","source":["%%writefile utils.py\n","import os\n","import openai\n","\n","def chunk_text(text: str, max_chars: int = 1000) -> list[str]:\n","    \"\"\"Split text into ≤ max_chars chunks, preserving paragraph breaks.\"\"\"\n","    paras, chunks, cur = text.split(\"\\n\\n\"), [], \"\"\n","    for p in paras:\n","        if len(cur) + len(p) + 2 <= max_chars:\n","            cur = (cur + \"\\n\\n\" + p).strip() if cur else p\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            if len(p) <= max_chars:\n","                cur = p\n","            else:\n","                for i in range(0, len(p), max_chars):\n","                    chunks.append(p[i : i + max_chars])\n","                cur = \"\"\n","    if cur:\n","        chunks.append(cur)\n","    return chunks\n","\n","def embed_text(text: str) -> list[float]:\n","    \"\"\"Get text embedding via OpenAI’s text-embedding-ada-002.\"\"\"\n","    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n","    resp = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n","    return resp[\"data\"][0][\"embedding\"]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsHpbOnUMTjP","executionInfo":{"status":"ok","timestamp":1745416576999,"user_tz":-330,"elapsed":50,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"4d791c1d-a93f-4471-bb11-ebf013c8c5dd"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing utils.py\n"]}]},{"cell_type":"code","source":["!mkdir agents # Create the 'agents' directory before writing the file"],"metadata":{"id":"ifyiuO0ygzuc","executionInfo":{"status":"ok","timestamp":1745416577065,"user_tz":-330,"elapsed":69,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["%%writefile agents/research_agent.py\n","# agents/research_agent.py\n","\n","import logging\n","from abc import ABC, abstractmethod\n","from tavily import TavilyClient\n","from tavily.errors import BadRequestError\n","\n","class Agent(ABC):\n","    @abstractmethod\n","    def execute(self, query: dict) -> dict:\n","        pass\n","\n","class ResearchAgent(Agent):\n","    def __init__(self, api_key: str):\n","        # TavilyClient for both extract and search\n","        self.client = TavilyClient(api_key=api_key)\n","\n","    def execute(self, query: dict) -> dict:\n","        \"\"\"\n","        1) If `seed_urls` present, use TavilyClient.extract to fetch raw_content.\n","        2) Else if `query[\"query\"]` present, use TavilyClient.search.\n","        Populates query[\"docs\"] = [{\"url\", \"title\", \"text\", \"images\"(opt)}, ...].\n","        \"\"\"\n","        docs = []\n","        try:\n","            seed_urls = query.get(\"seed_urls\", [])\n","            if seed_urls:\n","                logging.info(f\"[ResearchAgent] Extracting URLs via Tavily: {seed_urls}\")\n","                # Use Tavily's extract API (up to 20 URLs at once)\n","                resp = self.client.extract(\n","                    urls=seed_urls,\n","                    include_images=query.get(\"include_images\", False)\n","                )\n","                for item in resp.get(\"results\", []):\n","                    docs.append({\n","                        \"url\":   item.get(\"url\", \"\"),\n","                        \"title\": \"\",                  # extract() doesn’t return titles\n","                        \"text\":  item.get(\"raw_content\", \"\"),\n","                        \"images\": item.get(\"images\", [])\n","                    })\n","                # You could also check resp.get(\"failed_results\", []) here\n","\n","            else:\n","                terms = query.get(\"query\", \"\").strip()\n","                if not terms:\n","                    raise ValueError(\n","                        \"ResearchAgent requires either 'seed_urls' or a non-empty 'query'\"\n","                    )\n","                logging.info(f\"[ResearchAgent] Searching Tavily for: {terms}\")\n","                resp = self.client.search(\n","                    query=terms,\n","                    max_results=query.get(\"max_results\", 10),\n","                    include_raw_content=True\n","                )\n","                for item in resp.get(\"results\", []):\n","                    docs.append({\n","                        \"url\":   item.get(\"url\", \"\"),\n","                        \"title\": item.get(\"title\", \"\"),\n","                        \"text\":  item.get(\"raw_content\", \"\")\n","                    })\n","\n","        except BadRequestError as e:\n","            logging.error(f\"[ResearchAgent] Tavily BadRequestError: {e}\", exc_info=True)\n","        except Exception as e:\n","            logging.error(f\"[ResearchAgent] Unexpected error: {e}\", exc_info=True)\n","\n","        query[\"docs\"] = docs\n","        return query\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0LEWAtjMYJS","executionInfo":{"status":"ok","timestamp":1745416577229,"user_tz":-330,"elapsed":149,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"ebecf89c-55ff-4c44-bda4-bcbfb0d5e052"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing agents/research_agent.py\n"]}]},{"cell_type":"code","source":["%%writefile agents/translation_agent.py\n","from deep_translator import GoogleTranslator\n","from abc import ABC, abstractmethod\n","\n","# If you already have a `utils.chunk_text`, use that; otherwise this simple splitter will do:\n","def chunk_text(text: str, max_chars: int = 4500) -> list[str]:\n","    \"\"\"\n","    Split `text` into chunks no longer than max_chars, breaking on paragraph boundaries\n","    if possible.\n","    \"\"\"\n","    paras = text.split(\"\\n\\n\")\n","    chunks, cur = [], \"\"\n","    for p in paras:\n","        # +2 for the two-linebreaks we’d re‑insert\n","        if len(cur) + len(p) + 2 <= max_chars:\n","            cur = (cur + \"\\n\\n\" + p).strip() if cur else p\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            # if single paragraph is too long, hard‑split it\n","            if len(p) <= max_chars:\n","                cur = p\n","            else:\n","                for i in range(0, len(p), max_chars):\n","                    chunks.append(p[i : i + max_chars])\n","                cur = \"\"\n","    if cur:\n","        chunks.append(cur)\n","    return chunks\n","\n","\n","class Agent(ABC):\n","    @abstractmethod\n","    def execute(self, query: dict) -> dict:\n","        pass\n","\n","\n","class TranslationAgent(Agent):\n","    def __init__(self, max_chunk_size: int = 4500):\n","        # DeepTranslator enforces a hard max (around 5000 chars) so we leave a margin.\n","        self.max_chunk_size = max_chunk_size\n","\n","    def execute(self, query: dict) -> dict:\n","        text = query.get(\"text\", \"\") or \"\"\n","        # 1) Break into safe‑sized pieces\n","        chunks = chunk_text(text, max_chars=self.max_chunk_size)\n","        # 2) Translate each piece\n","        translated_chunks = []\n","        for chunk in chunks:\n","            translated_chunks.append(\n","                GoogleTranslator(source=\"auto\", target=\"en\").translate(chunk)\n","            )\n","        # 3) Re‑assemble\n","        query[\"translated_text\"] = \"\\n\\n\".join(translated_chunks)\n","        query[\"language\"] = \"auto\"\n","        return query\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKQPn_DzVPma","executionInfo":{"status":"ok","timestamp":1745416577247,"user_tz":-330,"elapsed":50,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"2981f95d-d69c-4250-cb27-e3fe49c10722"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing agents/translation_agent.py\n"]}]},{"cell_type":"code","source":["%%writefile agents/credibility_scorer.py\n","import whois, requests, xmltodict\n","from urllib.parse import urlparse\n","from datetime import datetime\n","from textblob import TextBlob\n","from transformers import pipeline\n","\n","# Zero‑shot fake vs real\n","_fake_news_clf = pipeline(\n","    \"zero-shot-classification\",\n","    model=\"facebook/bart-large-mnli\"\n",")\n","\n","class Agent:\n","    def execute(self, query: dict) -> dict:\n","        raise NotImplementedError\n","\n","class CredibilityScorer(Agent):\n","    def execute(self, query: dict) -> dict:\n","        url, text = query.get(\"url\", \"\"), query.get(\"text\", \"\")\n","        dom = urlparse(url).netloc\n","\n","        # Domain age\n","        try:\n","            w = whois.whois(dom)\n","            dates = w.creation_date\n","            date  = dates[0] if isinstance(dates, list) else dates\n","            age_yrs = (datetime.now() - date).days / 365\n","        except:\n","            age_yrs = None\n","\n","        # Alexa rank\n","        try:\n","            resp = requests.get(f\"https://data.alexa.com/data?cli=10&url={dom}\")\n","            doc  = xmltodict.parse(resp.text)\n","            alexa = int(doc[\"ALEXA\"][\"SD\"][1][\"POPULARITY\"][\"@TEXT\"])\n","        except:\n","            alexa = None\n","\n","        # Sentiment\n","        polarity = TextBlob(text).sentiment.polarity\n","\n","        # Fake‑news\n","        result = _fake_news_clf(text[:512], candidate_labels=[\"fake\", \"real\"])\n","        fn_label = result[\"labels\"][0]\n","        fn_score = float(result[\"scores\"][0])\n","\n","        query[\"credibility\"] = {\n","            \"https\":          urlparse(url).scheme == \"https\",\n","            \"domain_age_yrs\": age_yrs,\n","            \"alexa_rank\":     alexa,\n","            \"sentiment\":      polarity,\n","            \"fake_news\":      {\"label\": fn_label, \"score\": fn_score}\n","        }\n","        return query\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERI1MgTwMfmA","executionInfo":{"status":"ok","timestamp":1745416577264,"user_tz":-330,"elapsed":16,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"19093486-ee6b-4887-d666-c09c69d0bef5"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing agents/credibility_scorer.py\n"]}]},{"cell_type":"code","source":["%%writefile agents/graph_ingestor.py\n","# agents/graph_ingestor.py\n","\n","import logging\n","from langgraph_sdk import get_sync_client\n","from utils import chunk_text, embed_text\n","from agents.credibility_scorer import CredibilityScorer\n","import httpx\n","\n","class Agent:\n","    def execute(self, query: dict) -> dict:\n","        raise NotImplementedError\n","\n","class GraphIngestorWithCred(Agent):\n","    def __init__(self, endpoint: str, api_key: str):\n","        # You can keep passing whatever endpoint you like here;\n","        # we’ll catch errors at runtime.\n","        self.client = get_sync_client(url=endpoint, api_key=api_key)\n","        self.scorer = CredibilityScorer()\n","\n","    def execute(self, query: dict) -> dict:\n","        docs = query.get(\"docs\", [])\n","\n","        for doc in docs:\n","            # 1) Score credibility\n","            scored = self.scorer.execute(doc)\n","            doc_id = scored[\"url\"]\n","\n","            # 2) Try storing metadata\n","            try:\n","                self.client.store.put_item(\n","                    [\"documents\", \"meta\"],\n","                    doc_id,\n","                    {\n","                        \"url\":         scored[\"url\"],\n","                        \"title\":       scored.get(\"title\", \"\"),\n","                        \"language\":    scored.get(\"language\", \"\"),\n","                        \"credibility\": scored[\"credibility\"]\n","                    }\n","                )\n","            except Exception as e:\n","                logging.warning(f\"[GraphIngestor] Skipping metadata for {doc_id}: {e}\")\n","                # Skip embedding/chunking if metadata fails\n","                continue\n","\n","            # 3) Chunk & embed, then store each chunk\n","            text_to_chunk = scored.get(\"translated_text\") or scored.get(\"text\", \"\")\n","            for idx, chunk in enumerate(chunk_text(text_to_chunk)):\n","                try:\n","                    vec = embed_text(chunk)\n","                    self.client.store.put_item(\n","                        [\"documents\", \"chunks\"],\n","                        f\"{doc_id}#{idx}\",\n","                        {\n","                            \"content\":   chunk,\n","                            \"embedding\": vec\n","                        }\n","                    )\n","                except Exception as e:\n","                    logging.warning(\n","                        f\"[GraphIngestor] Skipping chunk {doc_id}#{idx}: {e}\"\n","                    )\n","                    # move on to next chunk\n","\n","        return query\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQJ1govjxozK","executionInfo":{"status":"ok","timestamp":1745416577366,"user_tz":-330,"elapsed":101,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"e529dda7-f8b6-4771-b842-c9a411d72c42"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing agents/graph_ingestor.py\n"]}]},{"cell_type":"code","source":["%%writefile agents/graph_retriever.py\n","# agents/graph_retriever.py\n","\n","import os\n","from langchain.schema import Document\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","from utils import chunk_text\n","\n","class Agent:\n","    def execute(self, query: dict) -> dict:\n","        raise NotImplementedError\n","\n","class GraphRetriever(Agent):\n","    def __init__(self, graph_client=None, persist_dir=\"graph_chroma\"):\n","        self.graph_client = graph_client\n","        embed_fn = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n","        self.vdb = Chroma(persist_directory=persist_dir, embedding_function=embed_fn)\n","\n","    def execute(self, query: dict) -> dict:\n","        docs = []\n","        if self.graph_client:\n","            entries = self.graph_client.store.search(\n","                namespace=[\"documents\",\"chunks\"], limit=1000\n","            )\n","            for e in entries:\n","                docs.append(Document(\n","                    page_content=e[\"value\"][\"content\"],\n","                    metadata={\"source_key\": e[\"key\"]}\n","                ))\n","        else:\n","            for doc in query.get(\"docs\", []):\n","                for idx, chunk in enumerate(chunk_text(doc.get(\"text\",\"\"))):\n","                    docs.append(Document(\n","                        page_content=chunk,\n","                        metadata={\"source\": doc.get(\"url\")}\n","                    ))\n","\n","        # add to Chroma\n","        self.vdb.add_documents(docs)\n","\n","        # ← HERE’S THE FIX: use .as_retriever() so you get a BaseRetriever\n","        query[\"retriever\"] = self.vdb.as_retriever()\n","        return query\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5LqYlbXMnKs","executionInfo":{"status":"ok","timestamp":1745416577377,"user_tz":-330,"elapsed":62,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"d6250e41-dc15-4476-d317-4bcab1020990"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing agents/graph_retriever.py\n"]}]},{"cell_type":"code","source":["%%writefile agents/drafting_agent.py\n","import logging\n","from abc import ABC, abstractmethod\n","from langchain.llms import OpenAI\n","from langchain.chains import RetrievalQA\n","\n","class Agent(ABC):\n","    @abstractmethod\n","    def execute(self, query: dict) -> dict:\n","        pass\n","\n","class DraftingAgent(Agent):\n","    def __init__(self, retriever, openai_api_key: str):\n","        self.chain = RetrievalQA.from_chain_type(\n","            llm=OpenAI(openai_api_key=openai_api_key, temperature=0.0),\n","            chain_type=\"stuff\",\n","            retriever=retriever,\n","            return_source_documents=True\n","        )\n","\n","    def execute(self, query: dict) -> dict:\n","        try:\n","            question = query.get(\"question\", \"\")\n","            logging.info(f\"[DraftingAgent] Generating answer for: {question}\")\n","            result = self.chain({\"query\": question})\n","            query[\"answer\"] = result[\"result\"]\n","            query[\"source_documents\"] = result[\"source_documents\"]\n","        except Exception as e:\n","            logging.error(f\"[DraftingAgent] Error: {e}\", exc_info=True)\n","            query[\"answer\"] = \"\"\n","            query[\"source_documents\"] = []\n","        return query\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FyVgugAWMqjp","executionInfo":{"status":"ok","timestamp":1745416577395,"user_tz":-330,"elapsed":31,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"ec0bcb00-b7a1-4779-d610-0b20fd752946"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing agents/drafting_agent.py\n"]}]},{"cell_type":"code","source":["%%writefile main.py\n","import os\n","from dotenv import load_dotenv\n","\n","from agents.research_agent    import ResearchAgent\n","from agents.translation_agent import TranslationAgent\n","from agents.graph_ingestor    import GraphIngestorWithCred\n","from agents.graph_retriever   import GraphRetriever\n","from agents.drafting_agent    import DraftingAgent\n","\n","from langgraph.sync import get_sync_client\n","\n","# Load API keys from .env\n","load_dotenv()\n","TAVILY_KEY    = os.getenv(\"TAVILY_API_KEY\")\n","LANGGRAPH_KEY = os.getenv(\"LANGGRAPH_API_KEY\")\n","OPENAI_KEY    = os.getenv(\"OPENAI_API_KEY\")\n","assert all([TAVILY_KEY, LANGGRAPH_KEY, OPENAI_KEY]), \"Missing one or more API keys\"\n","\n","# 1) Research using Tavily’s extract API\n","qry = {\n","    \"seed_urls\": [\n","        \"https://en.wikipedia.org/wiki/AI_ethics\",\n","        \"https://www.brookings.edu/research/principles-for-accountable-algorithms/\",\n","        \"https://www.technologyreview.com/2023/09/05/ai-ethics/\"\n","    ],\n","}\n","qry = ResearchAgent(api_key=TAVILY_KEY).execute(qry)\n","\n","# 2) Translate each snippet to English\n","for doc in qry[\"docs\"]:\n","    translated = TranslationAgent().execute({\"text\": doc[\"text\"]})\n","    doc[\"text\"] = translated[\"translated_text\"]\n","\n","# 3) Ingest into LangGraph (with credibility scoring)\n","#    Replace the URL below with your actual LangGraph endpoint.\n","GraphIngestorWithCred(\n","    endpoint=\"https://YOUR_LANGGRAPH_URL\",   # e.g. https://api.kairon.langgraph.cloud\n","    api_key=LANGGRAPH_KEY\n",").execute(qry)\n","\n","# 4) Build a sync-mode graph client and create your retriever\n","graph_client = get_sync_client(\n","    url=\"https://YOUR_LANGGRAPH_URL\",        # same URL here\n","    api_key=LANGGRAPH_KEY\n",")\n","gr        = GraphRetriever(graph_client=graph_client)\n","qry       = gr.execute(qry)\n","retriever = qry[\"retriever\"]\n","\n","# 5) Draft your answer\n","draft_q = {\n","    \"question\":  \"What are the key ethical concerns around AI according to recent discussions?\",\n","    \"retriever\": retriever\n","}\n","out = DraftingAgent(retriever, openai_api_key=OPENAI_KEY).execute(draft_q)\n","\n","# 6) Print\n","print(\"Answer:\\n\", out[\"answer\"])\n","print(\"\\nSources:\")\n","for src in out[\"source_documents\"]:\n","    key     = src.metadata.get(\"source_key\") or src.metadata.get(\"source\", \"unknown\")\n","    snippet = src.page_content.replace(\"\\n\", \" \")[:200]\n","    print(f\"- {key}: {snippet}…\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3U3ZPSlf3DiY","executionInfo":{"status":"ok","timestamp":1745416577451,"user_tz":-330,"elapsed":14,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"4ee83a54-cc27-4d43-ff07-e02e1ec7b2b4"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main.py\n"]}]},{"cell_type":"code","source":["!pip show langgraph\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MFVi9KvQwBOU","executionInfo":{"status":"ok","timestamp":1745416580814,"user_tz":-330,"elapsed":3352,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"363b9150-09ff-476c-ade2-cd925596b31b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: langgraph\n","Version: 0.3.31\n","Summary: Building stateful, multi-actor applications with LLMs\n","Home-page: \n","Author: \n","Author-email: \n","License: MIT\n","Location: /usr/local/lib/python3.11/dist-packages\n","Requires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk, xxhash\n","Required-by: \n"]}]},{"cell_type":"code","source":["!pip install --upgrade langgraph\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uK0MtRGKwRpy","executionInfo":{"status":"ok","timestamp":1745416587549,"user_tz":-330,"elapsed":6738,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"dcaf1467-b50b-45f8-be05-1d65f0472f00"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.31)\n","Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.55)\n","Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n","Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n","Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.63)\n","Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.31)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.3)\n","Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n","Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n","Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n"]}]},{"cell_type":"code","source":["%%bash\n","# 1) Install deps\n","pip install --quiet tavily-python langgraph-sdk langchain openai deep-translator beautifulsoup4 requests transformers textblob chromadb python-dotenv\n","\n","# 2) Run the test\n","python - << 'PYCODE'\n","import os, logging\n","from contextlib import suppress\n","from dotenv import load_dotenv\n","\n","from agents.research_agent    import ResearchAgent\n","from agents.translation_agent import TranslationAgent\n","from agents.graph_ingestor    import GraphIngestorWithCred\n","from agents.graph_retriever   import GraphRetriever\n","from agents.drafting_agent    import DraftingAgent\n","\n","# Enable INFO so we see ingestion warnings\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load keys explicitly\n","with suppress(Exception):\n","    load_dotenv(dotenv_path=\".env\")\n","\n","TAVILY_KEY    = os.getenv(\"TAVILY_API_KEY\")\n","LANGGRAPH_KEY = os.getenv(\"LANGGRAPH_API_KEY\")\n","OPENAI_KEY    = os.getenv(\"OPENAI_API_KEY\")\n","assert all([TAVILY_KEY, LANGGRAPH_KEY, OPENAI_KEY]), \"Missing one or more API keys\"\n","\n","# 1) Research\n","seed_urls = [\"https://en.wikipedia.org/wiki/Artificial_intelligence\"]\n","q = {\"seed_urls\": seed_urls}\n","q = ResearchAgent(api_key=TAVILY_KEY).execute(q)\n","assert q.get(\"docs\"), \"ResearchAgent returned no docs\"\n","\n","# 2) Translate\n","for doc in q[\"docs\"]:\n","    out = TranslationAgent().execute({\"text\": doc[\"text\"]})\n","    doc[\"text\"] = out[\"translated_text\"]\n","\n","# 3) Ingest (warnings OK) & fallback retriever\n","print(\"→ Ingesting (warnings expected if no LangGraph)…\")\n","_ = GraphIngestorWithCred(\n","    endpoint=\"https://this-does-not-exist.langgraph.io\",\n","    api_key=LANGGRAPH_KEY\n",").execute({\"docs\": q[\"docs\"]})\n","\n","retriever = GraphRetriever(graph_client=None).execute({\"docs\": q[\"docs\"]})[\"retriever\"]\n","\n","# 4) Draft\n","draft_agent = DraftingAgent(retriever, openai_api_key=OPENAI_KEY)\n","resp = draft_agent.execute({\n","    \"question\":  \"What is AI in one sentence?\",\n","    \"retriever\": retriever\n","})\n","\n","# 5) Output\n","print(\"\\n✅ Answer:\\n\", resp[\"answer\"])\n","print(\"\\n📑 Sources:\")\n","for s in resp[\"source_documents\"]:\n","    key     = s.metadata.get(\"source_key\") or s.metadata.get(\"source\") or \"unknown\"\n","    snippet = s.page_content.replace(\"\\n\", \" \")[:100]\n","    print(f\"- {key}: {snippet}…\")\n","PYCODE\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUyblTux6g8b","executionInfo":{"status":"ok","timestamp":1745416684392,"user_tz":-330,"elapsed":96846,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}},"outputId":"393d1c4a-adf9-4a3b-b039-143d2920c7ad"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["→ Ingesting (warnings expected if no LangGraph)…\n","\n","✅ Answer:\n","  AI is the capability of machines to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making.\n","\n","📑 Sources:\n","- https://en.wikipedia.org/wiki/Artificial_intelligence: al intelligence (AI) refers to the capability of computational systems to perform tasks typically as…\n","- https://en.wikipedia.org/wiki/Artificial_intelligence: From Wikipedia, the free encyclopedia Intelligence of machines \"AI\" redirects here. For other uses, …\n","- https://en.wikipedia.org/wiki/Artificial_intelligence: Vincent van Gogh in watercolour created by generative AI software These paragraphs are an excerpt fr…\n","- https://en.wikipedia.org/wiki/Artificial_intelligence: es, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analy…\n"]},{"output_type":"stream","name":"stderr","text":["2025-04-23 13:57:09.366776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1745416629.394786    1171 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1745416629.402856    1171 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-04-23 13:57:09.429530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","Device set to use cpu\n","/content/agents/graph_retriever.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n","\n",">> from langchain.embeddings import OpenAIEmbeddings\n","\n","with new imports of:\n","\n",">> from langchain_community.embeddings import OpenAIEmbeddings\n","You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n","  from langchain.embeddings.openai import OpenAIEmbeddings\n","/content/agents/graph_retriever.py:6: LangChainDeprecationWarning: Importing Chroma from langchain.vectorstores is deprecated. Please replace deprecated imports:\n","\n",">> from langchain.vectorstores import Chroma\n","\n","with new imports of:\n","\n",">> from langchain_community.vectorstores import Chroma\n","You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n","  from langchain.vectorstores import Chroma\n","/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n","\n","`from langchain_community.llms import OpenAI`.\n","\n","To install langchain-community run `pip install -U langchain-community`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n","\n","`from langchain_community.llms import OpenAI`.\n","\n","To install langchain-community run `pip install -U langchain-community`.\n","  warnings.warn(\n","INFO:root:[ResearchAgent] Extracting URLs via Tavily: ['https://en.wikipedia.org/wiki/Artificial_intelligence']\n","WARNING:root:[GraphIngestor] Skipping metadata for https://en.wikipedia.org/wiki/Artificial_intelligence: [Errno -2] Name or service not known\n","/content/agents/graph_retriever.py:16: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n","  embed_fn = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n","/content/agents/graph_retriever.py:17: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n","  self.vdb = Chroma(persist_directory=persist_dir, embedding_function=embed_fn)\n","INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n","INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n","/content/agents/drafting_agent.py:14: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n","  llm=OpenAI(openai_api_key=openai_api_key, temperature=0.0),\n","INFO:root:[DraftingAgent] Generating answer for: What is AI in one sentence?\n","/content/agents/drafting_agent.py:24: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  result = self.chain({\"query\": question})\n","INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n","INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n"]}]},{"cell_type":"code","source":["import os\n","# This will terminate the current Python process; Colab will automatically reconnect/restart it\n","os.kill(os.getpid(), 9)\n"],"metadata":{"id":"yzlU0woGHzT-"},"execution_count":null,"outputs":[]}]}